{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workingtocomment1commentbetter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFnHGwzF8Hqs",
        "outputId": "4d5af52d-8d5b-4d73-9fc0-f2e92c32c767"
      },
      "source": [
        "#Uncomment to use this code in Google Collab\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "\n",
        "#Importing Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import string\n",
        "import time\n",
        "import math\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlKUHLIZ8Jfh"
      },
      "source": [
        "#initiliazing Variables\n",
        "context_size = 2 #Dont Change\n",
        "embedded_size = 256\n",
        "batch_num = 500\n",
        "epoch = 3\n",
        "LR=0.01\n",
        "BATCH_SIZE = 1 #Dont Change\n",
        "BATCH_NUM = 10000\n",
        "DROPOUT = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zQ-oK2m8MZ3"
      },
      "source": [
        "#Models Class\n",
        "class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedded_size, context_size):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedded_size)\n",
        "        self.linear1 = nn.Linear(context_size * embedded_size, embedded_size)\n",
        "        self.linear2 = nn.Linear(embedded_size, vocab_size)\n",
        "        #Drop out function\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.dropout(out)\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovzillMr8Om0"
      },
      "source": [
        "#Batchifies the data for dataloader class\n",
        "def generate_batch(batch):\n",
        "\n",
        "    context = []    \n",
        "    target = []\n",
        "    for entry in batch:\n",
        "      for word in entry[0]:\n",
        "        #Handles unknown words from validation and testing data since they dont have tokens\n",
        "        if word in tokens:\n",
        "          context.append(tokens[word])\n",
        "        else:\n",
        "          context.append(tokens[\"UNK\"])\n",
        "      if entry[1] in tokens:\n",
        "        target.append(tokens[entry[1]])\n",
        "      else:\n",
        "        target.append(tokens[\"UNK\"])\n",
        "\n",
        "    label = torch.tensor(target, dtype=torch.long).cuda()\n",
        "    context = torch.tensor(context, dtype=torch.long).cuda()\n",
        "\n",
        "    return label,context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FzFAHYQ8Q26"
      },
      "source": [
        "#Reads Validation Data\n",
        "def eval_file():\n",
        "    #Use path for Google collab \"/content/gdrive/My Drive/nchlt_text.zu.valid\"\n",
        "    with open(\"nchlt_text.zu.valid\") as f:\n",
        "      batch_words_num = 0\n",
        "      words = []\n",
        "      trigrams = []\n",
        "      batch_counter = 0\n",
        "      batch = \"\"\n",
        "      for line in f:\n",
        "          #Removes Digits\n",
        "          line = ''.join([i for i in line if not i.isdigit()])\n",
        "          batch+=line.strip()+\" \"\n",
        "          if '.' in line:\n",
        "              batch_counter+=1\n",
        "              #Removes Punctuation\n",
        "              batch = batch.translate(str.maketrans('', '', string.punctuation))\n",
        "              allwords = batch.lower().split(\" \")\n",
        "              batch_words_num += len(allwords)\n",
        "              for word in allwords:\n",
        "                  words.append(word)\n",
        "              for i in range(len(words) - 2):\n",
        "                  trigrams.append(([words[i], words[i + 1]], words[i + 2]))\n",
        "              batch = \"\"\n",
        "              words=[]\n",
        "    return DataLoader(trigrams, batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4Q-9AAw8SrW"
      },
      "source": [
        "#Reads Training Data\n",
        "batch_counter = 0\n",
        "batch = \"\"\n",
        "vocabulary = {}\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lines_num = 0\n",
        "total_accu = None\n",
        "#Use path for Google collab \"/content/gdrive/My Drive/nchlt_text.zu.train\"\n",
        "with open(\"nchlt_text.zu.train\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        #Removes Digits\n",
        "        line = ''.join([i for i in line if not i.isdigit()])\n",
        "        #Removes Punctuation\n",
        "        line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "        words = line.lower().split(\" \")\n",
        "        #To create a token for unkown words we count how many times exists in data\n",
        "        for word in words:\n",
        "            if word in vocabulary:\n",
        "                vocabulary[word] += 1\n",
        "            else:\n",
        "                vocabulary[word] = 1\n",
        "        lines_num +=1\n",
        "    f.close()\n",
        "\n",
        "c=0\n",
        "tokens = {}\n",
        "index = {}\n",
        "#creating tokens for each word that occurs more than one and for others we say they are Unknown\n",
        "for word in vocabulary:\n",
        "    if vocabulary[word]>1:\n",
        "        c+=1\n",
        "        tokens[word] = c\n",
        "        index[c] = word\n",
        "tokens[\"UNK\"] = c+1\n",
        "index[c+1] = \"UNK\"\n",
        "\n",
        "#Preprocesing data with window method\n",
        "#Use path for Google collab \"/content/gdrive/My Drive/nchlt_text.zu.train\"\n",
        "with open(\"nchlt_text.zu.train\") as f:\n",
        "    batch_words_num = 0\n",
        "    words = []\n",
        "    trigrams = []\n",
        "    total_loss = 0\n",
        "    for line in f:\n",
        "        line = ''.join([i for i in line if not i.isdigit()])\n",
        "        batch+=line.strip()+\" \"\n",
        "        if '.' in line:\n",
        "            batch_counter+=1\n",
        "            batch = batch.translate(str.maketrans('', '', string.punctuation))\n",
        "            allwords = batch.lower().split(\" \")\n",
        "            batch_words_num += len(allwords)\n",
        "            for word in allwords:\n",
        "                if word in tokens:\n",
        "                    words.append(word)\n",
        "                else:\n",
        "                    words.append(\"UNK\")\n",
        "            for i in range(len(words) - 2):\n",
        "                trigrams.append(([words[i], words[i + 1]], words[i + 2]))\n",
        "            batch = \"\"\n",
        "            words=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xAPPCHQ6pRD"
      },
      "source": [
        "#To create new Model\n",
        "model = NGramLanguageModeler(len(vocabulary), embedded_size,context_size).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoK3myND6qWA"
      },
      "source": [
        "#To load Old Model\n",
        "#Use path for Google collab \"/content/gdrive/My Drive/model\"\n",
        "model = torch.load(\"model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ourS0Ovu8xIC",
        "outputId": "c79eb67d-651d-4c54-a807-9f29041a30bf"
      },
      "source": [
        "#initilizing variables for Model\n",
        "losses = []\n",
        "batch_c = 0\n",
        "#dataloader for training data\n",
        "dataloader = DataLoader(trigrams, batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)\n",
        "#Loss Function\n",
        "loss_function = nn.NLLLoss().cuda()\n",
        "#Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "#Changes Learning Rate according optimization\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "#dataloader for Validation data\n",
        "dataloader_valid = eval_file()\n",
        "#Use path for Google collab \"/content/gdrive/My Drive/log.txt\"\n",
        "file1 = open(\"log.txt\", \"w\")\n",
        "file1.write(\"Start Training\")\n",
        "file1.close()\n",
        "print(\"Start Training\")\n",
        "best_model = None\n",
        "best_val_loss = float(\"inf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC6Ie0ic87Uy",
        "outputId": "74e8d8db-925e-414e-83cf-7ea6a97cfcd2"
      },
      "source": [
        "#Start Training\n",
        "for epochnum in range(1,epoch+1):\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "    counter = 0\n",
        "    totalcount = 0\n",
        "    #Use path for Google collab \"/content/gdrive/My Drive/log.txt\"\n",
        "    file1 = open(\"log.txt\", \"a\")\n",
        "    #Going through batches\n",
        "    for label, text in enumerate(dataloader):\n",
        "        label = text[0]\n",
        "        text = text[1]\n",
        "        optimizer.zero_grad()\n",
        "        log_probs = model(text)\n",
        "        loss = loss_function(log_probs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        losses.append(total_loss)\n",
        "        counter+=1\n",
        "        #Prints out log file every BATCH_NUM size\n",
        "        if counter == BATCH_NUM:\n",
        "            end = time.time()\n",
        "            time_taken = end - start\n",
        "            totalcount +=counter\n",
        "            cur_loss = total_loss/totalcount\n",
        "            #Write to file\n",
        "            file1.write(\"\\n\")\n",
        "            file1.write('| epoch {:3d} | {:5d}/{:5d} batches | ' 'lr {:02.2f} | ms/batches {:5.2f} | ' 'loss {:5.2f} | ppl {:8.2f}'.format(epochnum, totalcount, len(dataloader) , LR,time_taken,cur_loss, math.exp(cur_loss)))\n",
        "            #System Write\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ' 'lr {:02.2f} | ms/batches {:5.2f} | ' 'loss {:5.2f} | ppl {:8.2f}'.format(epochnum, totalcount, len(dataloader) , LR,time_taken,cur_loss, math.exp(cur_loss)))\n",
        "            counter = 0\n",
        "            start = time.time()\n",
        "\n",
        "    #Evaluating model with validation data\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    start1 = time.time()\n",
        "    total_loss_eval = 0\n",
        "    with torch.no_grad():\n",
        "        #Going through validation data batches\n",
        "        for label, text in enumerate(dataloader_valid):\n",
        "            label = text[0]\n",
        "            text = text[1]\n",
        "            predited_label = model(text).cuda()\n",
        "            loss = loss_function(predited_label, label)\n",
        "            total_loss_eval += loss.item()\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    #Creating logs from results\n",
        "    time_taken = time.time() - start        \n",
        "    accu_val = total_acc/total_count\n",
        "    total_loss_eval = total_loss_eval/total_count\n",
        "    ppl = math.exp(total_loss_eval)\n",
        "    print(\"End of Epoch \"+str(epochnum))\n",
        "    print(\"Evaluation for validation data : \"+str(accu_val))\n",
        "    print(\"Time : \" +str(time_taken))\n",
        "    print(\"loss : \"+str(total_loss_eval))\n",
        "    print(\"ppl : \"+str(ppl))\n",
        "    file1.write(\"\\nEnd of Epoch \"+str(epochnum))\n",
        "    file1.write(\"\\nEvaluation for validation data : \"+str(accu_val))\n",
        "    file1.write(\"\\nTime : \" +str(time_taken))\n",
        "    file1.write(\"\\nloss : \"+str(total_loss_eval))\n",
        "    file1.write(\"\\nppl : \"+str(ppl))\n",
        "    #Changing Learning rate\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "        scheduler.step()\n",
        "    else:\n",
        "        total_accu = accu_val\n",
        "    total_loss = 0\n",
        "    #Use path for Google collab \"/content/gdrive/My Drive/model\"\n",
        "    torch.save(model, \"model\")\n",
        "    file1.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MliKUfPoNi_a"
      },
      "source": [
        "#Reading Test file and preprocessing\n",
        "def test_file():\n",
        "    #Use path for Google collab \"/content/gdrive/My Drive/nchlt_text.zu.test\"\n",
        "    with open(\"nchlt_text.zu.test\") as f:\n",
        "      batch_words_num = 0\n",
        "      words = []\n",
        "      trigrams = []\n",
        "      batch_counter = 0\n",
        "      batch = \"\"\n",
        "      for line in f:\n",
        "          #Remove digits\n",
        "          line = ''.join([i for i in line if not i.isdigit()])\n",
        "          batch+=line.strip()+\" \"\n",
        "          if '.' in line:\n",
        "              batch_counter+=1\n",
        "              #Remove punctuation\n",
        "              batch = batch.translate(str.maketrans('', '', string.punctuation))\n",
        "              allwords = batch.lower().split(\" \")\n",
        "              batch_words_num += len(allwords)\n",
        "              for word in allwords:\n",
        "                  words.append(word)\n",
        "              for i in range(len(words) - 2):\n",
        "                  trigrams.append(([words[i], words[i + 1]], words[i + 2]))\n",
        "              batch = \"\"\n",
        "              words=[]\n",
        "              \n",
        "    return DataLoader(trigrams, batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y50wWs_mNsIm"
      },
      "source": [
        "#Use path for Google collab \"/content/gdrive/My Drive/log.txt\"\n",
        "file1 = open(\"log.txt\", \"a\")\n",
        "dataloader_test = test_file()\n",
        "model.eval()\n",
        "total_acc, total_count = 0, 0\n",
        "start = time.time()\n",
        "total_loss_test = 0\n",
        "#Evaluating model with test data\n",
        "with torch.no_grad():\n",
        "    for label, text in enumerate(dataloader_test):\n",
        "        label = text[0]\n",
        "        text = text[1]\n",
        "        predited_label = model(text).cuda()\n",
        "        loss = loss_function(predited_label, label)\n",
        "        total_loss_test += loss.item()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "#Creating logs from results\n",
        "time_taken = time.time() - start        \n",
        "accu_test = total_acc/total_count\n",
        "total_loss_test = total_loss_test/total_count\n",
        "ppl = math.exp(total_loss_test)\n",
        "print(\"Evaluation for TEST data : \"+str(accu_test))\n",
        "print(\"Time : \" +str(time_taken))\n",
        "print(\"loss : \"+str(total_loss_test))\n",
        "print(\"ppl : \"+str(ppl))\n",
        "file1.write(\"\\nEvaluation for TEST data : \"+str(accu_test))\n",
        "file1.write(\"\\nTime : \" +str(time_taken))\n",
        "file1.write(\"\\nloss : \"+str(total_loss_test))\n",
        "file1.write(\"\\nppl : \"+str(ppl))\n",
        "file1.close()\n",
        "#Use path for Google collab \"/content/gdrive/My Drive/model\"\n",
        "torch.save(model, \"model\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
